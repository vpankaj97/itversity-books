{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platforms to Practice\n",
    "\n",
    "Let us understand different platforms we can leverage to practice Apache Spark using Python.\n",
    "\n",
    "* Local Setup\n",
    "* Using ITVersity labs\n",
    "* Databricks Platform\n",
    "* Setting up your own cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark Locally - Ubuntu\n",
    "\n",
    "Let us setup Spark Locally on Ubuntu.\n",
    "\n",
    "* Install latest version of Anaconda\n",
    "* Make sure Jupyter Notebook is setup and validated.\n",
    "* Setup Spark and Validate.\n",
    "* Setup Environment Variables to integrate Pyspark with Jupyter Notebook.\n",
    "* Launch Jupyter Notebook using `pyspark` command.\n",
    "* Setup PyCharm (IDE) for application development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark Locally - Mac\n",
    "\n",
    "* Install latest version of Anaconda\n",
    "* Make sure Jupyter Notebook is setup and validated.\n",
    "* Setup Spark and Validate.\n",
    "* Setup Environment Variables to integrate Pyspark with Jupyter Notebook.\n",
    "* Launch Jupyter Notebook using `pyspark` command.\n",
    "* Setup PyCharm (IDE) for application development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signing up for ITVersity Labs\n",
    "\n",
    "Let us understand how to sign up for ITVersity labs to get access to the content as well environment to practice.\n",
    "* Go to https://labs.itversity.com\n",
    "* Sign up for the website and purchase the plan\n",
    "* Create account in the labs and then use credentials in lab page to login to Jupyter based environment.\n",
    "* Following are the advantages using labs:\n",
    "  * Unified experience - content, cluster.\n",
    "  * Support using Slack.\n",
    "  * No headache in setting up environment for practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ITVersity Labs\n",
    "\n",
    "Let us understand how to submit the Spark Jobs in ITVersity Labs.\n",
    "\n",
    "* As we are using Python we can also use the help command to get the documentation - for example `help(spark.read.csv)`\n",
    "* You can practice either using terminal or by using Jupyter Notebook directly.\n",
    "* You need to choose appropriate kernel to run the code leveraging Spark.\n",
    "* To access terminal you need to launch terminal from File -> New -> New Terminal.\n",
    "* You can use `pyspark2` command to launch Pyspark in terminal and then to practice.\n",
    "* Example to copy paste in the terminal.\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "pyspark2 --master yarn --conf spark.ui.port=0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with File Systems\n",
    "\n",
    "When it comes to on-prem clusters such as our labs one can use `hdfs` commands to interact with file system. However it is different for Databricks Platform. \n",
    "\n",
    "Let us understand how to interact with file system using %fs command from Databricks Notebook.\n",
    "\n",
    "* We can access datasets using `%fs` magic command in Databricks notebook\n",
    "* By default, we will see files under dbfs\n",
    "* We can list the files using ls command - e. g.: `(%fs ls)`\n",
    "* Databricks provides lot of datasets for free under databricks-datasets\n",
    "* If the cluster is integrated with AWS or Azure Blob we can access files by specifying the appropriate protocol (e.g.: s3:// for s3)\n",
    "* List of commands available under %fs\n",
    " * Copying files or directories `cp`\n",
    " * Moving files or directories `mv`\n",
    " * Creating directories `mkdirs`\n",
    " * Deleting files and directories `rm`\n",
    " * We can copy or delete directories recursively using `-r` or `--recursive`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting File Metadata\n",
    "\n",
    "Let us review the source location to get number of files and the size of the data we are going to process.\n",
    "\n",
    "* Location of airlines data dbfs:/databricks-datasets/airlines\n",
    "* We can get first 1000 files using %fs ls dbfs:/databricks-datasets/airlines\n",
    "* Location contain 1919 Files, however we will not be able to see all the details using %fs command.\n",
    "* Databricks File System commands does not have capability to understand metadata of files such as size in details.\n",
    "* When Spark Cluster is started, it will create 2 objects - spark and sc\n",
    "* sc is of type SparkContext and spark is of type SparkSession\n",
    "* Spark uses HDFS APIs to interact with the file system and we can access HDFS APIs using sc._jsc and sc._jvm to get file metadata.\n",
    "* Here are the steps to get the file metadata.\n",
    " * Get Hadoop Configuration using ` sc._jsc.hadoopConfiguration()` - let's say `conf`\n",
    "  * We can pass conf to `sc._jvm.org.apache.hadoop.fs.FileSystem` get to get FileSystem object - let's say `fs`\n",
    "  * We can build `path`  object by passing the path as string to `sc._jvm.org.apache.hadoop.fs.Path`\n",
    "  * We can invoke `listStatus` on top of fs by passing path which will return an array of FileStatus objects - let's say files.  \n",
    "  * Each `FileStatus` object have all the metadata of each file.\n",
    "  * We can use `len` on files to get number of files.\n",
    "  * We can use `getLen` on each `FileStatus` object to get the size of each file. \n",
    "  Cumulative size of all files can be achieved using `sum(map(lambda file: file.getLen(), files))`\n",
    "  \n",
    "Let us first get list of files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls dbfs:/databricks-datasets/airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is the `hdfs` command to run from terminal.\n",
    "\n",
    "```\n",
    "hdfs dfs -ls /public/airlines_all\n",
    "hdfs dfs -ls /public/airlines_all/airlines\n",
    "hdfs dfs -ls /public/airlines_all/airlines_part\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess. \\\n",
    "    check_output(['hdfs', 'dfs', '-ls', '/public/airlines_all']). \\\n",
    "    splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess. \\\n",
    "    check_output(['hdfs', 'dfs', '-ls', '/public/airlines_all/airlines']). \\\n",
    "    splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the consolidated script to get number of files and cumulative size of all files in a given folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    master('yarn'). \\\n",
    "    appName('Computing Airlines Data Size'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = sc._jsc.hadoopConfiguration()\n",
    "fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks\n",
    "# path = sc._jvm.org.apache.hadoop.fs.Path(\"dbfs:/databricks-datasets/airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITVersity labs\n",
    "path = sc._jvm.org.apache.hadoop.fs.Path('/public/airlines_all/airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = fs.listStatus(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(sum(map(lambda file: file.getLen(), files))/1024/1024/1024, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also get the size of a folder by using Hadoop commands such as `hdfs dfs`.\n",
    "\n",
    "```\n",
    "hdfs dfs -du -s -h /public/airlines_all/airlines\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.check_output(['hdfs', 'dfs', '-du', '-s', '-h', '/public/airlines_all/airlines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* You can either use command line or programmatic approach.\n",
    "* Get size of **/public/airlines_all/airlines_part**\n",
    "* Get size of **/public/retail_db**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
