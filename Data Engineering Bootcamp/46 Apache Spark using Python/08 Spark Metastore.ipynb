{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Metastore\n",
    "\n",
    "Let us understand how to interact with metastore tables using Spark based APIs.\n",
    "\n",
    "* Overview of Spark Metastore\n",
    "* Starting Spark Context\n",
    "* Spark Catalog\n",
    "* Creating Metastore Tables\n",
    "* Define Schema for Tables\n",
    "* Inserting into Existing Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark Metastore\n",
    "\n",
    "Let us get an overview of Spark Metastore and how we can leverage it to manage databases and tables on top of Big Data based file systems such as HDFS, s3 etc.\n",
    "\n",
    "* Quite often we need to deal with structured data and the most popular way of processing structured data is by using Databases, Tables and then SQL.\n",
    "* Spark Metastore (similar to Hive Metastore) will facilitate us to manage databases and tables.\n",
    "* Typically Metastore is setup using traditional relational database technologies such as **Oracle**, **MySQL**, **Postgres** etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark Context\n",
    "\n",
    "Let us start spark context for this Notebook so that we can execute the code provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName('Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Catalog\n",
    "\n",
    "Let us get an overview of Spark Catalog. It is part of `SparkSession` object.\n",
    "* We can permanently or temporarily create tables or views on top of data in a Data Frame.\n",
    "* Metadata such as table names, column names, data types etc for these tables or views will be stored in Metastore. It is also known as catalog which is exposed as part of SparkSession object.\n",
    "* Permanent tables can be grouped into databases in metastore. If not specified, the tables will be created in **default** database.\n",
    "* Let us say `spark` is of type `SparkSession`. There is an attribute as part of `spark` called as catalog and it is of type pyspark.sql.catalog.Catalog.\n",
    "* We can access catalog using `spark.catalog`.\n",
    "* There are several methods that is part of `spark.catalog`. We will explore them in the later topics.\n",
    "* Following are some of the tasks that can be performed using `spark.catalog` object.\n",
    " * Check current database and switch to different databases.\n",
    " * Create permanent table in metastore.\n",
    " * Create or drop temporary views.\n",
    " * Register functions.\n",
    "* All the above tasks can be performed using SQL style commands passed to `spark.sql`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Metastore Tables\n",
    "\n",
    "Data Frames can be written into Metastore Tables using APIs such as `saveAsTable` and `insertInto` available as part of write on top of objects of type Data Frame.\n",
    "\n",
    "* We can create a new table using Data Frame using `saveAsTable`. We can also create an empty table by using `spark.catalog.createTable` or `spark.catalog.createExternalTable`.\n",
    "* We can also prefix the database name to write data into tables belong to a particular database. If the database is not specified then the session will be attached to default database.\n",
    "* Databases can be created using `spark.sql(\"CREATE DATABASE database_name\")`. We can list Databases using `spark.sql` or `spark.catalog.listDatabases()`\n",
    "* We can use modes such as `append`, `overwrite` and `error` with `saveAsTable`. Default is error.\n",
    "* We can use modes such as `append` and `overwrite` with `insertInto`. Default is append.\n",
    "* When we use `saveAsTable`, following happens:\n",
    " * Check for table if the table already exists. By default `saveAsTable` will throw exception.\n",
    " * If the table does not exists the table will be created.\n",
    " * Data from Data Frame will be copied into the table.\n",
    " * We can alter the behavior by using mode. We can overwrite the existing table or we can append into it.\n",
    "* We can list the tables using `spark.catalog.listTables` after switching to appropriate database using `spark.catalog.setCurrentDatabase`.\n",
    "* We can also switch the database and list tables using `spark.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "Let us perform few tasks to understand how to write a Data Frame into Metastore tables and also list them.\n",
    "* Create database by name db in the metastore. We need to use `spark.sql` as there is no function to create database under `spark.catalog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE {username}_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List the databases using both API as well as SQL approach. As we have too many databases in our environment, it might take too much time to return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a Data Frame which contain one column by name **dummy** and one row with value **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(\"X\", )]\n",
    "df = spark.createDataFrame(l, schema=\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a table by name dual for the above Data Frame in the database created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(\"dual\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us drop the table **dual** and then database **db**. We need to use `spark.sql` as `spark.catalog` does not have API to drop the tables or databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP DATABASE {username}_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use CASCADE to drop database along with tables.\n",
    "spark.sql(f\"DROP DATABASE {username}_db CASCADE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Schema for Tables\n",
    "\n",
    "When we want to create a table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`, we need to specify Schema.\n",
    "\n",
    "* Schema can be inferred or we can pass schema using `StructType` object while creating the table..\n",
    "* `StructType` takes list of objects of type `StructField`.\n",
    "* `StructField` is built using column name and data type. All the data types are available under `pyspark.sql.types`.\n",
    "* We need to pass table name and schema for `spark.catalog.createTable`.\n",
    "* We have to pass path along with name and schema for `spark.catalog.createExternalTable`.\n",
    "* We can use source to define file format along with applicable options. For example, if we want to create a table for CSV, then source will be csv and we can pass applicable options for CSV such as sep, header etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform tasks to create empty table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`.\n",
    "\n",
    "* Create database **hr_db** and table **employees** with following fields. Let us create Database first and then we will see how to create table.\n",
    " * employee_id of type Integer\n",
    " * first_name of type String\n",
    " * last_name of type String\n",
    " * salary of type Float\n",
    " * nationality of type String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build StructType object using StructField list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, \\\n",
    "    IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesSchema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"nationality\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE employees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create table by passing StructType object as schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.createTable(\"employees\", schema=employeesSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List the tables from database created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listColumns('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESCRIBE FORMATTED employees').show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create database by name **{username}_airlines** and create external table for **airport-codes.txt**.\n",
    " * Data have header\n",
    " * Fields in each record are delimited by a tab character.\n",
    " * We can pass options such as sep, header, inferSchema etc to define the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.createExternalTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_airlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To create external table, we need to have write permissions over the path which we want to use.\n",
    "* As we have only read permissions on **/public/airlines_all/airport-codes** we cannot use that path while creating external table.\n",
    "* Let us copy the data to **/user/`whoami`/airlines_all/airport-codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/`whoami`/airlines_all\n",
    "hdfs dfs -cp -f /public/airlines_all/airport-codes /user/`whoami`/airlines_all\n",
    "hdfs dfs -ls /user/`whoami`/airlines_all/airport-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -tail /user/`whoami`/airlines_all/airport-codes/airport-codes-na.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "airport_codes_path = f'/user/{username}/airlines_all/airport-codes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE airport_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog. \\\n",
    "    createExternalTable(\"airport_codes\",\n",
    "                        path=airport_codes_path,\n",
    "                        source=\"csv\",\n",
    "                        sep=\"\\t\",\n",
    "                        header=\"true\",\n",
    "                        inferSchema=\"true\"\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(\"airport_codes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESCRIBE FORMATTED airport_codes').show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting into Existing Tables\n",
    "\n",
    "Let us understand how we can insert data into existing tables using `insertInto`.\n",
    "\n",
    "* We can use modes such as `append` and `overwrite` with `insertInto`. Default is `append`.\n",
    "* When we use `insertInto`, following happens:\n",
    " * If the table does not exist, `insertInto` will throw an exception.\n",
    " * If the table exists, by default data will be appended.\n",
    " * We can alter the behavior by using keyword argument overwrite. It is by default False, we can pass True to replace existing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform few tasks to understand how to write a Data Frame into existing tables in the Metastore.\n",
    "\n",
    "* Make sure hr_db database and employees table in hr_db are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use employees Data Frame and insert data into the employees table in hr_db database. Make sure existing data is overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "             (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "             (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "             (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "    schema=\"\"\"employee_id INT, first_name STRING, last_name STRING,\n",
    "              salary FLOAT, nationality STRING\n",
    "           \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.write.insertInto(\"employees\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.write.mode('overwrite').insertInto(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(\"employees\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
