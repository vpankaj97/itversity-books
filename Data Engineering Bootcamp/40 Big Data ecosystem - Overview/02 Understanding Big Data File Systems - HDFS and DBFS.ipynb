{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Big Data File Systems - HDFS and DBFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get an overview of File Systems that are used as part of Big Data Clusters, both on-prem as well as cloud.\n",
    "\n",
    "* Understanding Storage Servers\n",
    "* List of File Systems\n",
    "* Understanding Hadoop Storage (HDFS)\n",
    "* HDFS Architecture\n",
    "* HDFS Commands – Overview\n",
    "* Customizing Properties\n",
    "* Overview of DBFS Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Storage Servers\n",
    "\n",
    "Let us understand details about Storage Servers.\n",
    "\n",
    "* A computer is nothing but CPU, Memory and Storage. Even though CPU and Memory are important for making application usable as they are transient we cannot rely on them for the data which need to be permanently stored. Hard Drives store data permanently and hence we use Hard Drives (or SSDs) to permanently store the data.\n",
    "\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/05/Screenshot-2018-12-13-at-8.01.21-AM.png)\n",
    "\n",
    "* Earlier we used to have CPU, Memory and Storage on the same server and used to take periodic back ups so that we can restore data in case of Hardware failures. But restoring Hard Drives require downtime of the applications.\n",
    "* Hence, Storage is decoupled from the servers and Storage Racks or Storage Servers are evolved (EMC is one of the pioneers in this space).\n",
    "* Storage Servers or Racks are connected to actualy servers via high speed fiber optic networking.\n",
    "* The storage rack is like a container with a CPU, Memory etc. We can plug in many hard drives in the rack. Software installed in the server that comes as part of rack takes care of the distribution of files across hard drives, load balancing, fault tolerance etc.\n",
    "\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/05/Screenshot-2018-12-13-at-8.02.05-AM-768x443.png)\n",
    "\n",
    "* Let us take an example here:\n",
    "    * Each drive is 2 TB in size\n",
    "    * Let us say we have 8 such hard drives, then we have 16 TB of total storage.\n",
    "    * With RAID 0, we will achieve the distribution of files across all the hard drives. However, even if we lose one hard drive, then there will be an outage for all the applications which are using storage on that storage rack.\n",
    "    * Distribution of the files on multiple hard drives is called as striping.\n",
    "    * To make Hard Drives fault tolerant we use Mirroring or configure at different RAID levels.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of File Systems\n",
    "\n",
    "Let us get list of file systems that are available on both on-prem clusters as well as cloud.\n",
    "* On-prem - HDFS, Open Source S3 (Minio)\n",
    "* AWS Native - S3\n",
    "* Azure Native - Blob, ADLS\n",
    "* Databricks - DBFS\n",
    "\n",
    "It is very easy to learn about file systems as a developer or data engineer.\n",
    "\n",
    "Here are the tasks we typically perform:\n",
    "* Create Directories and/or Buckets (on Cloud).\n",
    "* Copy files from local file system into the File Systems and vice versa.\n",
    "* Delete unnecessary files.\n",
    "* List the details of files and folders.\n",
    "* We can perform most of the operations using CLI or web based interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Hadoop Storage (HDFS)\n",
    "\n",
    "* HDFS stands for Hadoop Distributed File System.\n",
    "    * Distributed\n",
    "    * Fault-Tolerant\n",
    "    * Highly Reliable\n",
    "* On a regular local file system such as your PC, a file will occupy contiguous blocks of storage. However, in the network file system, blocks of a file need not be contiguous. They might spread across multiple Hard Drives that are part of Storage Server.\n",
    "* Let us first copy data set and understand what is going on under the hood.\n",
    "    * Files will be divided into blocks\n",
    "    * Blocks will be stored in multiple nodes\n",
    "    * There will be multiple copies of each block\n",
    "* Instead of the separate storage server, HDFS is designed to use local file system itself as part of Distributed File System. We will have multiple servers as part of HDFS.\n",
    "* In our cluster, we have worker nodes on which Datanodes are running. These Datanodes are managed by Namenode(s).\n",
    "* Data is typically stored in the form of blocks on the servers where Datanode is running. Block Size is by default 128 MB.\n",
    "* As compared to RAID in legacy systems, in Hadoop Replication takes care of data reliability.\n",
    "* By default, Hadoop creates 3 replicas, it maintains 3 copies of each block.\n",
    "* **Rack awareness** – In Hadoop, data is stored in rack aware fashion. It means that one block on one rack and other two blocks on another rack. This makes cluster more reliable.\n",
    "* Replication factor and rack awareness give fault-tolerance to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -du -s -h /public/randomtextwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls -h /public/randomtextwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs fsck /public/randomtextwriter/part-m-00029 -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Architecture\n",
    "\n",
    "* There are three HDFS daemons – Namenode (which is the master daemon) and Datanodes (which are slave daemons) and Secondary Namenode.\n",
    "* Namenode and all the Datanodes are connected to the network switch.\n",
    "* hdfs fsck command gives metadata information (like file permissions etc.)\n",
    "* Namenode stores the metadata information.\n",
    "* Datanodes store the actual data.\n",
    "* The client interacts with Namenode and finds out where the file blocks are stored.\n",
    "* Data can be recovered using edit logs and FSImage. Edit logs is a file structure which is a transaction log. FSImage is a snapshot of data at a particular time.\n",
    "* Secondary name node keeps merging Edit Log and FSImage into a new FSimage. This process of merging Edit Log and FSImage is known as Checkpointing.\n",
    "* By default, every 1-second Datanode sends heartbeat to Namenode. If Namenode doesn’t receive any heartbeat from Datanode for pre-configured time, it will be discarded and no more blocks will be copied to that Datanode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Commands – Overview\n",
    "\n",
    "Now let us walk through some of the important commands we use as developers on regular basis.\n",
    "\n",
    "* We can get usage of all commands using **hadoop fs** or **hadoop fs -usage**\n",
    "* We can get usage of a single command using hadoop fs -usage COMMAND\n",
    "    * Usage for ls – **hadoop fs -usage ls**\n",
    "* We can get the help of all commands using **hadoop fs -help**\n",
    "* We can get help for a single command using **hadoop fs -help COMMAND**\n",
    "    * Help for ls – **hadoop fs -help ls**\n",
    "* List all the files in HDFS – **hadoop fs -ls /user/training**\n",
    "* **hdfs dfs** command is an alias for **hadoop fs**.\n",
    "* We can copy data from local file system to HDFS using **hadoop fs -put** or **hadoop fs -copyFromLocal**\n",
    "    * Source is from local file system /data/cards\n",
    "    * Target is in HDFS /user/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -usage ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -help ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/`whoami`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -help put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -ltr /data/cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -put /data/cards /user/`whoami`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/`whoami`/cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To see the contents of a smaller file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -cat /user/`whoami`/cards/smalldeck.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To tail larger files to preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -tail /user/`whoami`/cards/smalldeck.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Updating a file is not possible in hdfs\n",
    "* To append contents of local file /data/cards/smalldeck.txt to /user/training/cards/largedeck.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -appendToFile /data/cards/smalldeck.txt /user/`whoami`/cards/largedeck.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* copyFromLocal is similar to put command. copyTolocal and get command are similar and are used to copy files from hdfs to local file system.\n",
    "\n",
    "These few commands are good enough for our rest of the course. However, if you are interested in learning all the commands go to the next section where several other important HDFS Commands are covered in detail. It is self-paced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Properties\n",
    "\n",
    "\n",
    "Let us see how we can override properties while copying data into HDFS.\n",
    "\n",
    "* Under the /etc directory, there will be one directory for each service. Hadoop is a combination of two services, HDFS, and YARN along with Map Reduce.\n",
    "* In /etc/hadoop/conf directory we have xml files like core-site.xml, hdfs-site.xml, mapred-site.xml, and yarn-site.xml\n",
    "* core-site.xml has properties for both the components of Hadoop.\n",
    "* hdfs-site.xml has properties like block size. By default, the block size is 128 MB.\n",
    "* To override block size while copying the file from the local file system to HDFS from 128MB to 64 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls -R /public/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs fsck /public/crime/csv/crime_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -Ddfs.blocksize=67108864 -put /data/crime/csv/rows.csv  /user/`whoami`/crime_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs fsck /user/`whoami`/crime_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also override the replication factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop fs -Ddfs.replication=2 -put /data/crime/csv/rows.csv  /user/`whoami`/crime_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Getting Metadata Information\n",
    "\n",
    "* The files are divided into the block and each block in turn divided into locations. So, using fsck command we can get the metadata information of a particular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs fsck /data/crime/csv/rows.csv -files - blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Here is the exercise on some important HDFS Commands.\n",
    "\n",
    "* Copy data from /data/retail_db to your userspace in HDFS /user/YOUR_USERNAME/retail_db\n",
    "* /data/retail_db have 6 sub directories and each sub directory have exactly one file.\n",
    "* List the files recursively to validate that /user/YOUR_USERNAME/retail_db have exactly similar directory structure as /data/retail_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of DBFS\n",
    "Let us also understand about Databricks File System.\n",
    "* Databricks is the distribution that is available both on AWS as well as Azure.\n",
    "* We can manage files and folders using Databricks Web Interface.\n",
    "* We can setup databricks CLI and should be able to perform tasks such as copy files, create directories as well as delete them.\n",
    "\n",
    "|Commands |Description|\n",
    "|---------|-----------|\n",
    "|  cat  |      Show the contents of a file.|\n",
    "|  configure | Configures host and authentication info for the CLI.|\n",
    "|  cp     |    Copy files to and from DBFS.|\n",
    "|  ls     |    List files in DBFS.|\n",
    "|  mkdirs |    Make directories in DBFS.|\n",
    "|  mv     |    Moves a file between two DBFS paths.|\n",
    "|  rm     |    Remove files from dbfs.|\n",
    "\n",
    "Let us look at the commands to copy files into Databricks platform using Databricks CLI. We will also see how we can review the details using `%fs` magic command in Databricks Notebook.\n",
    "\n",
    "```\n",
    "databricks fs configure --token \n",
    "# Enter URL and token provided by Databricks platform\n",
    "\n",
    "databricks fs cp -r /Users/itversity/Research/data/retail_db dbfs:/FileStore/retail_db\n",
    "\n",
    "databricks fs ls dbfs:/FileStore\n",
    "\n",
    "databricks fs rm dbfs:/FileStore/retail_db --recursive\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
