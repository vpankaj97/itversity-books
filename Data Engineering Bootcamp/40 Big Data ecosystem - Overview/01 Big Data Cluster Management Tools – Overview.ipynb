{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Cluster Management Tools – Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this lesson, we will be primarily focusing on getting started with Hadoop and Spark.\n",
    "\n",
    "* Overview of Big Data cluster\n",
    "* Reviewing Cluster using Cluster Management Tools\n",
    "* HDFS – Hadoop Distributed File System\n",
    "* YARN, Map Reduce and Spark Overview\n",
    "* Overview of Configuration Files\n",
    "* Apache Spark - Cluster Types\n",
    "* Overview - Running Spark Jobs\n",
    "\n",
    "Map Reduce and Spark are two alternative Distributed Processing Frameworks. We will be covering Spark in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview about Ambari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the details one need to understand about management tools such as Ambari.\n",
    "\n",
    "* Ambari Overview\n",
    "* Ambari Architecture\n",
    "* Hosts and Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambari Overview\n",
    "\n",
    "Sign in on Ambari, which is a cluster management tool. It is provided by Hortonworks. \n",
    "* To get the details at hardware level go to hosts tab and to get the details at software level ,go to services tab.\n",
    "* Services are softwares deployed on the cluster.It can also show trends in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambari Architecture\n",
    "\n",
    "Let us understand the architecture of Ambari\n",
    "* It follows Agent and server architecture.There is a centralised server such as Ambari server and it uses a centralised database.Someone has to give information on each node in the cluster to the server.\n",
    "* Check for services running on Ambari-\n",
    "`ps -ef|grep -i ambari`\n",
    "* Ambari agent and server can be seen running there on the host.Agent will capture certain metrics for every service deployed (how much CPU,memory or HDFS etc.)and pass information to server and it will store it into database and give information in form of graph and charts.\n",
    "* In an Ambari cluster of say 1000 nodes, most of the nodes run agents and few will run server. So, in a typical cluster we have master nodes based on the number of master services we need to run, worker nodes and gateway nodes which helps us to connect with cluster. Gateway nodes deploy the jobs in production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosts and Services\n",
    "\n",
    "Let us get the details about the cluster using Ambari.\n",
    "* We have Hosts and Services\n",
    "* Hosts are nothing but servers on which cluster is setup. They are broadly categorized into gateways, masters and workers.\n",
    "* Services are nothing but softwares deployed on the cluster.\n",
    "  * HDFS\n",
    "  * YARN\n",
    "  * Spark\n",
    "  * Hive\n",
    "  * and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS – Hadoop Distributed File System\n",
    "\n",
    "Let us understand more about File System (HDFS) in the cluster. \n",
    "* HDFS stands for Hadoop Distributed File System.\n",
    "* File System determines the process of storing files in File System.\n",
    "* The hard disk is a collection of bits. Every operating system divides the hard disk into blocks and has some default block size. When we need to store some file on hard disk, it will be stored depending upon the block size. For example, if the default block size is 4 KB, to store 100KB file it stores sequentially taking 25 blocks.\n",
    "* Due to fragmentation, sometimes even if space is available it can’t store file.\n",
    "* In the case of Distributed file systems, we can store files non-sequentially and even on different nodes also.\n",
    "* Now, go to Gateway node, list the files present in the local file system using ls command. Go to data and crime directory and check the information about file size. It needs to store sequentially.\n",
    "* To interact with HDFS, we can use hadoop fs command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN, Map Reduce and Spark Overview\n",
    "\n",
    "While HDFS is Distributed File System, we need to have frameworks to process the data.\n",
    "\n",
    "* YARN is primarily for Resource Management\n",
    "* Map Reduce is the core distributed computing framework as part of Hadoop. However, it have lost its prime lately.\n",
    "* Spark is the most important distributed computing framwork. It can be plugged into different execution frameworks and YARN is one of them.\n",
    "\n",
    "We will get into details about Spark as part of the dedicated modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Configuration Files\n",
    "\n",
    "Let us get an overview of configuration files.\n",
    "* Each and every service will have configuration files associated with it.\n",
    "* Here is the mapping between the service and the location of its corresponding configuration files.\n",
    "  * HDFS - /etc/hadoop/conf\n",
    "  * YARN - /etc/hadoop/conf\n",
    "  * Spark - /etc/spark/conf or /etc/spark2/conf\n",
    "  * Hive - /etc/hive/conf\n",
    "* Configuration files might be in xml format or in properties files format.\n",
    "* We can review the properties files using Management Tools such as Ambari, Cloudera Manager etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark - Cluster Types\n",
    "\n",
    "Spark can run on several cluster managers.\n",
    "* YARN\n",
    "  * Spark on on-prem clusters leveraging distributions use YARN.\n",
    "  * It is supported by Cloudera, Hortonworks as well as MapR distributions.\n",
    "  * Spark on cloud native clusters such as AWS EMR, GCP Dataproc runs using YARN. These cloud native services comes with Hadoop which includes YARN.\n",
    "* Mesos\n",
    "* Standalone\n",
    "  * Databricks uses standalone and it is available both on AWS as well as Azure.\n",
    "* Kubernetes\n",
    "* Local (Development)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview - Running Spark Jobs\n",
    "\n",
    "Let us understand how to run Spark Jobs on different environments.\n",
    "\n",
    "* On-Prem\n",
    "  * Running Spark Jobs using interactive shells\n",
    "  * Running Spark Jobs using Notebook environments (JupyterHub)\n",
    "  * Submitting Spark Jobs using spark-submit\n",
    "  * Submitting Spark Jobs using livy\n",
    "* AWS EMR\n",
    "  * Using AWS EMR Web Console\n",
    "  * Using command line (AWS CLI)\n",
    "* Databricks\n",
    "  * Using Databricks Web interface\n",
    "  * Using command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Jobs - Architectural Patterns\n",
    "\n",
    "Let us understand how we typically develop, implement and manage Spark Jobs against different environments.\n",
    "\n",
    "* On-Prem - Preview using ITVersity Labs\n",
    "  * Clusters will always be running\n",
    "  * Develop the code (Read, Process and Write data).\n",
    "  * Build code bundle (jar or zip).\n",
    "  * Deploy on gateway node.\n",
    "  * Schedule using scheduling tools like AirFlow.\n",
    "* Cloud - Preview using Databricks\n",
    "  * Cluster might be in stopped state or might be running with minimum capacity. Set thresholds for auto scale up and down.\n",
    "  * Develop the code (Read, Process and Write data).\n",
    "  * Build code bundle (jar or zip).\n",
    "  * Deploy using scheduler (using external scheduler tools or vendor provided scheduling capabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
