{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Getting Started with Spark\n",
    "\n",
    "As part of this module we will take a simple use case and try to scratch the surface of the Spark. We will be using simple use case to demontrate end to end Data Engineering Pipeline.\n",
    "\n",
    "* Understand Data Model\n",
    "* Define Problem Statement\n",
    "* Creating Spark Context\n",
    "* Setting Run Time Job Properties\n",
    "* Reading data from CSV Files\n",
    "* Apply Filtering\n",
    "* Row Level Transformations\n",
    "* Perform Joins\n",
    "* Aggregate Data\n",
    "* Perform Sorting\n",
    "* Write output to Files\n",
    "* Complete Script\n",
    "* Validating Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Data Model\n",
    "\n",
    "Let us understand the data model and also characteristics of the data.\n",
    "\n",
    "* Base directory for **retail_db** data sets is **/public/retail_db**.\n",
    "* It have six folders, each folder represents a separate table.\n",
    "  * Product Catalog Tables\n",
    "    * products\n",
    "    * categories\n",
    "    * departments\n",
    "  * Customers Table\n",
    "    * customers\n",
    "  * Transactional Tables\n",
    "    * orders\n",
    "    * order_items\n",
    "* **orders** and **order_items** are related. **orders** is parent table and **order_items** is child table for orders.\n",
    "* All folders have one ore more files under them.\n",
    "* Each line represents a record and have values related to multiple columns. Each record is delimited or separated by **comma (,)**.\n",
    "* First field in each orders record is order_id and it is a primary key (unique and not null)\n",
    "* Second field in each order_items record is order_item_order_id which is a foreign key attribute to orders order_id.\n",
    "* There are other relationships as well, however they are not relevant to get started. We will primarily focus on orders and order_items data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem Statement\n",
    "\n",
    "Get monthly revenue considering complete or closed orders\n",
    "\n",
    "* We will use orders and order_items data.\n",
    "* **orders** is available at **/public/retail_db/orders**\n",
    "* **order_items** is available at **/public/retail_db/order_items**\n",
    "* We need to consider orders with COMPLETE or CLOSED status.\n",
    "* Revenue can be computed using **order_item_subtotal** which is 5th attribute in order_items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Context\n",
    "\n",
    "Let us understand how to create Spark Context using `SparkSession` from `pyspark.sql`.\n",
    "\n",
    "* We need to have spark context to leverage both APIs as well as distributed computing framework.\n",
    "* `SparkSession` is a wrapper class which will use existing Spark Context or create new one.\n",
    "* We can customize the behavior of Spark Context created by passing properties using `config` or by using APIs such as `appName`, `master` etc.\n",
    "* APIs are provided only for most commonly used properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark as fs\n",
    "import os\n",
    "fs.init()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('Getting Started - Monthly Revenue'). \\\n",
    "    master('local'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x6812da0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-249MU1B.mshome.net:55537\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Getting Started - Monthly Revenue</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Run Time Job Properties\n",
    "\n",
    "Let us understand how to customize run time behavior of submitted jobs.\n",
    "\n",
    "* Once Spark Context is created, we can customize run time behavior by using `spark.conf.set`. \n",
    "* In our case let us set a property called as `spark.sql.shuffle.partitions` to 2.\n",
    "* If we do not set this property, by default it will use 200 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When using Jupyter Notebook, if you want to improvise the readability of the data of the show command then you can set `spark.sql.repl.eagerEval.enabled` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from CSV Files\n",
    "\n",
    "Let us quickly see how we can read data from CSV Files.\n",
    "\n",
    "* Spark provide several APIs to read the files of different file formats.\n",
    "* All the out of the box APIs are available under `spark.read`.\n",
    "* In our case we have to read text files where each record is delimited or separated by comma (',').\n",
    "* To create Data Frames for `orders` and `order_items` we can pass the path to `spark.read.csv`.\n",
    "* There are other options as well which can be passed using keyword arguments. You can run help on `spark.read.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mSignature:\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mescape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnullValue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnanValue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositiveInf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegativeInf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdateFormat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestampFormat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxColumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxCharsPerColumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxMalformedLogPerPartition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiLine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforceSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memptyValue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m\nLoads a CSV file and returns the result as a  :class:`DataFrame`.\n\nThis function will go through the input once to determine the input schema if\n``inferSchema`` is enabled. To avoid going through the entire data once, disable\n``inferSchema`` option or specify the schema explicitly using ``schema``.\n\n:param path: string, or list of strings, for input path(s),\n             or RDD of Strings storing CSV rows.\n:param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n               or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n:param sep: sets a single character as a separator for each field and value.\n            If None is set, it uses the default value, ``,``.\n:param encoding: decodes the CSV files by the given encoding type. If None is set,\n                 it uses the default value, ``UTF-8``.\n:param quote: sets a single character used for escaping quoted values where the\n              separator can be part of the value. If None is set, it uses the default\n              value, ``\"``. If you would like to turn off quotations, you need to set an\n              empty string.\n:param escape: sets a single character used for escaping quotes inside an already\n               quoted value. If None is set, it uses the default value, ``\\``.\n:param comment: sets a single character used for skipping lines beginning with this\n                character. By default (None), it is disabled.\n:param header: uses the first line as names of columns. If None is set, it uses the\n               default value, ``false``.\n:param inferSchema: infers the input schema automatically from data. It requires one extra\n               pass over the data. If None is set, it uses the default value, ``false``.\n:param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n                      forcibly applied to datasource files, and headers in CSV files will be\n                      ignored. If the option is set to ``false``, the schema will be\n                      validated against all headers in CSV files or the first header in RDD\n                      if the ``header`` option is set to ``true``. Field names in the schema\n                      and column names in CSV headers are checked by their positions\n                      taking into account ``spark.sql.caseSensitive``. If None is set,\n                      ``true`` is used by default. Though the default value is ``true``,\n                      it is recommended to disable the ``enforceSchema`` option\n                      to avoid incorrect results.\n:param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n                                values being read should be skipped. If None is set, it\n                                uses the default value, ``false``.\n:param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n                                 values being read should be skipped. If None is set, it\n                                 uses the default value, ``false``.\n:param nullValue: sets the string representation of a null value. If None is set, it uses\n                  the default value, empty string. Since 2.0.1, this ``nullValue`` param\n                  applies to all supported types including the string type.\n:param nanValue: sets the string representation of a non-number value. If None is set, it\n                 uses the default value, ``NaN``.\n:param positiveInf: sets the string representation of a positive infinity value. If None\n                    is set, it uses the default value, ``Inf``.\n:param negativeInf: sets the string representation of a negative infinity value. If None\n                    is set, it uses the default value, ``Inf``.\n:param dateFormat: sets the string that indicates a date format. Custom date formats\n                   follow the formats at ``java.text.SimpleDateFormat``. This\n                   applies to date type. If None is set, it uses the\n                   default value, ``yyyy-MM-dd``.\n:param timestampFormat: sets the string that indicates a timestamp format. Custom date\n                        formats follow the formats at ``java.text.SimpleDateFormat``.\n                        This applies to timestamp type. If None is set, it uses the\n                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n:param maxColumns: defines a hard limit of how many columns a record can have. If None is\n                   set, it uses the default value, ``20480``.\n:param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n                          value being read. If None is set, it uses the default value,\n                          ``-1`` meaning unlimited length.\n:param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n                                    If specified, it is ignored.\n:param mode: allows a mode for dealing with corrupt records during parsing. If None is\n             set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n             parse only required columns in CSV under column pruning. Therefore, corrupt\n             records can be different based on required set of fields. This behavior can\n             be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n             (enabled by default).\n\n        * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n          into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n          fields to ``null``. To keep corrupt records, an user can set a string type \\\n          field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n          schema does not have the field, it drops corrupt records during parsing. \\\n          A record with less/more tokens than schema is not a corrupted record to CSV. \\\n          When it meets a record having fewer tokens than the length of the schema, \\\n          sets ``null`` to extra fields. When the record has more tokens than the \\\n          length of the schema, it drops extra tokens.\n        * ``DROPMALFORMED`` : ignores the whole corrupted records.\n        * ``FAILFAST`` : throws an exception when it meets corrupted records.\n\n:param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n                                  created by ``PERMISSIVE`` mode. This overrides\n                                  ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n                                  it uses the value specified in\n                                  ``spark.sql.columnNameOfCorruptRecord``.\n:param multiLine: parse records, which may span multiple lines. If None is\n                  set, it uses the default value, ``false``.\n:param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n                                  the quote character. If None is set, the default value is\n                                  escape character when escape and quote characters are\n                                  different, ``\\0`` otherwise.\n:param samplingRatio: defines fraction of rows used for schema inferring.\n                      If None is set, it uses the default value, ``1.0``.\n:param emptyValue: sets the string representation of an empty value. If None is set, it uses\n                   the default value, empty string.\n\n>>> df = spark.read.csv('python/test_support/sql/ages.csv')\n>>> df.dtypes\n[('_c0', 'string'), ('_c1', 'string')]\n>>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n>>> df2 = spark.read.csv(rdd)\n>>> df2.dtypes\n[('_c0', 'string'), ('_c1', 'string')]\n\n.. versionadded:: 2.0\n\u001b[1;31mFile:\u001b[0m      c:\\pyspark\\python\\pyspark\\sql\\readwriter.py\n\u001b[1;31mType:\u001b[0m      instancemethod\n"
    }
   ],
   "source": [
    "spark.read.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading orders\n",
    "orders_path = '/public/retail_db/orders'\n",
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path, \n",
    "        schema=\"order_id INT, order_date STRING, \" +\n",
    "               \"order_customer_id INT, order_status STRING\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading orders\n",
    "orders_path = \"D://Bigdata Tutorials//data//retail_db//orders\"\n",
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path, \n",
    "        schema=\"order_id INT, order_date STRING, \" +\n",
    "               \"order_customer_id INT, order_status STRING\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- order_id: integer (nullable = true)\n |-- order_date: string (nullable = true)\n |-- order_customer_id: integer (nullable = true)\n |-- order_status: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------+--------------------+-----------------+---------------+\n|order_id|          order_date|order_customer_id|   order_status|\n+--------+--------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n+--------+--------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also set `spark.sql.repl.eagerEval.enabled` to `True` and then just run Data Frame name as part of Jupyter Notebook Cell to preview the data in the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "+--------+--------------------+-----------------+---------------+\n|order_id|          order_date|order_customer_id|   order_status|\n+--------+--------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n+--------+--------------------+-----------------+---------------+\nonly showing top 20 rows",
      "text/html": "<table border='1'>\n<tr><th>order_id</th><th>order_date</th><th>order_customer_id</th><th>order_status</th></tr>\n<tr><td>1</td><td>2013-07-25 00:00:...</td><td>11599</td><td>CLOSED</td></tr>\n<tr><td>2</td><td>2013-07-25 00:00:...</td><td>256</td><td>PENDING_PAYMENT</td></tr>\n<tr><td>3</td><td>2013-07-25 00:00:...</td><td>12111</td><td>COMPLETE</td></tr>\n<tr><td>4</td><td>2013-07-25 00:00:...</td><td>8827</td><td>CLOSED</td></tr>\n<tr><td>5</td><td>2013-07-25 00:00:...</td><td>11318</td><td>COMPLETE</td></tr>\n<tr><td>6</td><td>2013-07-25 00:00:...</td><td>7130</td><td>COMPLETE</td></tr>\n<tr><td>7</td><td>2013-07-25 00:00:...</td><td>4530</td><td>COMPLETE</td></tr>\n<tr><td>8</td><td>2013-07-25 00:00:...</td><td>2911</td><td>PROCESSING</td></tr>\n<tr><td>9</td><td>2013-07-25 00:00:...</td><td>5657</td><td>PENDING_PAYMENT</td></tr>\n<tr><td>10</td><td>2013-07-25 00:00:...</td><td>5648</td><td>PENDING_PAYMENT</td></tr>\n<tr><td>11</td><td>2013-07-25 00:00:...</td><td>918</td><td>PAYMENT_REVIEW</td></tr>\n<tr><td>12</td><td>2013-07-25 00:00:...</td><td>1837</td><td>CLOSED</td></tr>\n<tr><td>13</td><td>2013-07-25 00:00:...</td><td>9149</td><td>PENDING_PAYMENT</td></tr>\n<tr><td>14</td><td>2013-07-25 00:00:...</td><td>9842</td><td>PROCESSING</td></tr>\n<tr><td>15</td><td>2013-07-25 00:00:...</td><td>2568</td><td>COMPLETE</td></tr>\n<tr><td>16</td><td>2013-07-25 00:00:...</td><td>7276</td><td>PENDING_PAYMENT</td></tr>\n<tr><td>17</td><td>2013-07-25 00:00:...</td><td>2667</td><td>COMPLETE</td></tr>\n<tr><td>18</td><td>2013-07-25 00:00:...</td><td>1205</td><td>CLOSED</td></tr>\n<tr><td>19</td><td>2013-07-25 00:00:...</td><td>9488</td><td>PENDING_PAYMENT</td></tr>\n<tr><td>20</td><td>2013-07-25 00:00:...</td><td>9198</td><td>PROCESSING</td></tr>\n</table>\nonly showing top 20 rows\n"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading order_items\n",
    "order_items_path = '/public/retail_db/order_items'\n",
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    csv(order_items_path, \n",
    "        schema=\"order_item_id INT, order_item_order_id INT, \" +\n",
    "               \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "               \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading order_items\n",
    "order_items_path = \"D://Bigdata Tutorials//data//retail_db//order_items\"\n",
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    csv(order_items_path, \n",
    "        schema=\"order_item_id INT, order_item_order_id INT, \" +\n",
    "               \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "               \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- order_item_id: integer (nullable = true)\n |-- order_item_order_id: integer (nullable = true)\n |-- order_item_product_id: integer (nullable = true)\n |-- order_item_quantity: integer (nullable = true)\n |-- order_item_subtotal: float (nullable = true)\n |-- order_item_product_price: float (nullable = true)\n\n"
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n|            1|                  1|                  957|                  1|             299.98|                  299.98|\n|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n|            3|                  2|                  502|                  5|              250.0|                    50.0|\n|            4|                  2|                  403|                  1|             129.99|                  129.99|\n|            5|                  4|                  897|                  2|              49.98|                   24.99|\n|            6|                  4|                  365|                  5|             299.95|                   59.99|\n|            7|                  4|                  502|                  3|              150.0|                    50.0|\n|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n|            9|                  5|                  957|                  1|             299.98|                  299.98|\n|           10|                  5|                  365|                  5|             299.95|                   59.99|\n|           11|                  5|                 1014|                  2|              99.96|                   49.98|\n|           12|                  5|                  957|                  1|             299.98|                  299.98|\n|           13|                  5|                  403|                  1|             129.99|                  129.99|\n|           14|                  7|                 1073|                  1|             199.99|                  199.99|\n|           15|                  7|                  957|                  1|             299.98|                  299.98|\n|           16|                  7|                  926|                  5|              79.95|                   15.99|\n|           17|                  8|                  365|                  3|             179.97|                   59.99|\n|           18|                  8|                  365|                  5|             299.95|                   59.99|\n|           19|                  8|                 1014|                  4|             199.92|                   49.98|\n|           20|                  8|                  502|                  1|               50.0|                    50.0|\n+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Filtering\n",
    "\n",
    "Let us see how we can filter out records in Data Frame.\n",
    "* We can either use `filter` or `where` to filter the data. Both of them serve the same purpose.\n",
    "* We can pass the condictions either in SQL Style or API Style.\n",
    "* In this case, we have used SQL Style to check `order_status` for `COMPLETE` or `CLOSED` orders.\n",
    "* We can perform all standard filtering conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered = orders. \\\n",
    "    filter('order_status in (\"COMPLETE\", \"CLOSED\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example for API Style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    filter(orders.order_status.isin('COMPLETE', 'CLOSED')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Level Transformations\n",
    "\n",
    "Let us see how we can project and also derive new fields out of existing fields leveraging functions.\n",
    "\n",
    "* One of the ways to project data is by using `select` on top of Data Frame.\n",
    "* Spark provides almost 300 pre defined functions as part of `pyspark.sql.functions`.\n",
    "* In our case we need to import and use `date_format` function to extract month from existing date.\n",
    "* Later we will also import and use functions such as `sum` and `round` while aggregating the data.\n",
    "* We can also provide meaningful names to derived fields using `alias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "orders_transformed = orders_filtered. \\\n",
    "    select('order_id', date_format('order_date', 'yyyyMM').alias('order_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Joins\n",
    "\n",
    "Let us join both the data sets which have the fields we are interested in. \n",
    "\n",
    "* We can join data sets using `join`.\n",
    "* We also might have to pass join condition in case the column names are different between the data sets.\n",
    "* In our case we have to join `orders` and `order_items` using `orders.order_id` and `order_items.order_item_order_id`.\n",
    "\n",
    "We can join original Data Frames as well and generate order_month while grouping the data as demonstrated in the **Complete Script** Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month = orders_transformed. \\\n",
    "    join(order_items, \n",
    "         orders.order_id == order_items.order_item_order_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Data\n",
    "\n",
    "As we have joined `orders` and `order_items`, let us perform the aggregation.\n",
    "* In this case want to compute revenue for each month.\n",
    "* `order_month` is derived field which contain both year and month.\n",
    "* We can use `order_month` as part of `groupBy` so that data can be grouped. It will generate a special Data Frame of type `GroupedData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month. \\\n",
    "    groupBy('order_month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can now invoke aggregate functions such as `sum` and pass the desired field using which we want to aggregate (in this case we can pass `order_item_subtotal` to `sum`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "monthly_revenue = order_details_by_month. \\\n",
    "    groupBy('order_month'). \\\n",
    "    agg(round(sum('order_item_subtotal'), 2).alias('revenue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sorting\n",
    "\n",
    "As we got the revenue for each month, let us sort the data so that we can review the output for the validation.\n",
    "\n",
    "* We can use `orderBy` or `sort` to sort the data.\n",
    "* By default data will be sorted in ascending order.\n",
    "* In this case we are sorting the data by `order_month`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue_sorted = monthly_revenue. \\\n",
    "    orderBy('order_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output to Files\n",
    "\n",
    "As data is read, processed and sorted - now it is time to write data to files in underlying file system.\n",
    "\n",
    "* In our environment **/public** is read only folder. You will not be able to add files under subdirectories of **/public**.\n",
    "* Assuming you have write access to **/user/[OS_USER_NAME]**, I have used **/user/{username}/retail_db/monthly_revenue** as target folder.\n",
    "* `{username}` is replaced by the OS user used for login using `getpass.getuser()`.\n",
    "* `coalesce(1)` is used to write the output to one file.\n",
    "* If the folder and files already exists, `mode('overwrite')` will replace existing folder with new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "monthly_revenue_sorted. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    csv(f'/user/{username}/retail_db/monthly_revenue',\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Script\n",
    "\n",
    "Here is the complete script or program which takes care of the following:\n",
    "\n",
    "* Create Spark Context and set the properties.\n",
    "* Read the data related to different tables.\n",
    "* Process the data using relevant Spark Data Frame APIs.\n",
    "* Write the data back to file system\n",
    "\n",
    "Entire Data processing and writing the data back to file system is developed using Piped approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format, sum, round\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName('Getting Started - Monthly Revenue'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "# Reading orders\n",
    "orders_path = '/public/retail_db/orders'\n",
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path, \n",
    "        schema=\"order_id INT, order_date STRING, \" +\n",
    "               \"order_customer_id INT, order_status STRING\"\n",
    "       )\n",
    "\n",
    "# Reading order_items\n",
    "order_items_path = '/public/retail_db/order_items'\n",
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    csv(order_items_path, \n",
    "        schema=\"order_item_id INT, order_item_order_id INT, \" +\n",
    "               \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "               \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "       )\n",
    "\n",
    "orders. \\\n",
    "    filter('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "    join(order_items, orders.order_id == order_items.order_item_order_id). \\\n",
    "    groupBy(date_format('order_date', 'yyyyMM').alias('order_month')). \\\n",
    "    agg(round(sum('order_item_subtotal'), 2).alias('revenue')). \\\n",
    "    orderBy('order_month'). \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    csv(f'/user/{username}/retail_db/monthly_revenue',\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Output\n",
    "\n",
    "Let us go ahead and validate the output.\n",
    "\n",
    "* In our case we are using HDFS and hence we should be able to use HDFS commands to validate.\n",
    "* Let us first list the files which will give some idea about when they are created.\n",
    "* For some file formats, we will also see extension as well as compression algorithm used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/monthly_revenue/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In case of small text files we can use `cat` to see the contents. It might not work if the files are compressed.\n",
    "* Also, it is not a good practice to use `cat` for larger text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -cat /user/`whoami`/retail_db/monthly_revenue/part*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.16-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}